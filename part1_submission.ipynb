{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import surprise\n",
    "from surprise import SVD, SVDpp, NMF\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import evaluate, print_perf\n",
    "from surprise import KNNBasic\n",
    "from surprise import AlgoBase, BaselineOnly\n",
    "from get_top_n import get_top_n\n",
    "from surprise.accuracy import rmse, mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>productID</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>370183</th>\n",
       "      <td>A2OR4QUQSUMOW7</td>\n",
       "      <td>B0016B9FSU</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2013-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370186</th>\n",
       "      <td>A14E7LZASLSX36</td>\n",
       "      <td>B0016B9FSU</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2013-03-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370189</th>\n",
       "      <td>A9ESHA5MS6S6L</td>\n",
       "      <td>B0016B9FSU</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2013-01-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370192</th>\n",
       "      <td>ATATZGNDHA5ZD</td>\n",
       "      <td>B0016B9FSU</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2013-02-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370193</th>\n",
       "      <td>AN4HRAGRHHX1H</td>\n",
       "      <td>B0016B9FSU</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2013-01-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            reviewerID   productID  rating        date\n",
       "370183  A2OR4QUQSUMOW7  B0016B9FSU     4.0  2013-01-31\n",
       "370186  A14E7LZASLSX36  B0016B9FSU     5.0  2013-03-21\n",
       "370189   A9ESHA5MS6S6L  B0016B9FSU     5.0  2013-01-17\n",
       "370192   ATATZGNDHA5ZD  B0016B9FSU     5.0  2013-02-05\n",
       "370193   AN4HRAGRHHX1H  B0016B9FSU     5.0  2013-01-03"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data\n",
    "dt = pd.read_csv('sample_data.csv',index_col=0)\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Construct missing ratings we need to predict\n",
    "select_data = dt.pivot_table('rating',index=['reviewerID','productID'],dropna=False)\n",
    "select_data = select_data.loc[select_data['rating'].isnull()]\n",
    "select_data.reset_index(inplace=True)\n",
    "missing_values = select_data[['reviewerID','productID']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1,5))\n",
    "dt = Dataset.load_from_df(dt[['reviewerID','productID','rating']],reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_ratings = dt.raw_ratings\n",
    "#Shuffle ratings\n",
    "random.seed(42)\n",
    "random.shuffle(raw_ratings)\n",
    "#Split to training and test sets\n",
    "threshold = int(.8 * len(raw_ratings))\n",
    "A_raw_ratings = raw_ratings[:threshold]\n",
    "B_raw_ratings = raw_ratings[threshold:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt.raw_ratings = A_raw_ratings  # dt is now the set A - training set\n",
    "dt.split(n_folds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Baseline model: User's bias & item's bias\n",
    "class BaselineModel(AlgoBase):\n",
    "\n",
    "    def __init__(self):\n",
    "        AlgoBase.__init__(self)\n",
    "\n",
    "    def compute_bias(self):\n",
    "        bu = np.zeros(self.trainset.n_users)\n",
    "        bi = np.zeros(self.trainset.n_items)\n",
    "        global_mean = self.trainset.global_mean\n",
    "        \n",
    "        for i in self.trainset.all_items():\n",
    "            dev_i = 0\n",
    "            for (u, r) in self.trainset.ir[i]:\n",
    "                dev_i += r - global_mean\n",
    "            bi[i] = dev_i/(len(self.trainset.ir[i]))\n",
    "        \n",
    "        for u in self.trainset.all_users():\n",
    "            dev_u = 0\n",
    "            for (i, r) in self.trainset.ur[u]:\n",
    "                dev_u += r - global_mean\n",
    "            bu[u] = dev_u/(len(self.trainset.ur[u]))\n",
    "        return bu, bi\n",
    "    \n",
    "    def train(self, trainset):\n",
    "        AlgoBase.train(self, trainset)\n",
    "        self.bu, self.bi = self.compute_bias()\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        est = self.trainset.global_mean\n",
    "        if self.trainset.knows_user(u):\n",
    "            est += self.bu[u]\n",
    "        if self.trainset.knows_item(i):\n",
    "            est += self.bi[i]\n",
    "        return est\n",
    "\n",
    "algo1 = BaselineModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Construct train set and test set for use by surprise\n",
    "trainset = dt.build_full_trainset()\n",
    "testset = dt.construct_testset(B_raw_ratings)  # testset is now the set B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train and predict using baseline model\n",
    "algo1.train(trainset)\n",
    "baseline_predictions = algo1.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.0607\n",
      "MAE:  0.7961\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.79607315037876514"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy on test set\n",
    "rmse(baseline_predictions)\n",
    "mae(baseline_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9572\n",
      "MAE:  0.7484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.74837625011915421"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVD\n",
    "algo2 = SVD()\n",
    "algo2.train(trainset)\n",
    "svd_predictions = algo2.test(testset)\n",
    "rmse(svd_predictions)\n",
    "mae(svd_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to calculate coverage ratio\n",
    "def coverage_ratio(predictions, n):\n",
    "    top_n = get_top_n(predictions, n)\n",
    "    product_list = []\n",
    "    recommended_list = []\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        product_list.append(iid)\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        recommended_list.append(user_ratings[0][0])\n",
    "    coverage = float(len(set(recommended_list))) / float(len(set(product_list)))\n",
    "    return coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9428571428571428"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coverage_ratio(svd_predictions,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to predict missing values based on an algorithm\n",
    "def mv_prediction(algo,missing_values):\n",
    "    predictions = [algo.predict(uid, iid)\n",
    "                       for (uid, iid) in missing_values]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mv_prediction_list = mv_prediction(algo2,missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22857142857142856"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coverage_ratio(mv_prediction_list,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use a default knn config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define algorithm config\n",
    "algo_name = KNNBasic ##  KNNWithMeans,KNNBaseline\n",
    "sim_option={'name': 'cosine', ## cosine, msd, pearson, personbaseline\n",
    "                 'user_based': 'False', ## False for item-based\n",
    "                 'min_surpport':0 }##  if |Iuv|<min_support then sim(u,v)=0\n",
    "max_k = 40 ## The (max) number of neighbors to take into account for aggregation\n",
    "min_k = 1 ##  If there are not enough neighbors, the prediction is set the the global mean of all ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_default = algo_name(k = max_k, min_k = min_k, sim_options=sim_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "knn_default.train(trainset)\n",
    "defaultknn_prediction=knn_default.test(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm KNNBasic.\n",
      "\n",
      "------------\n",
      "Fold 1\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9879\n",
      "MAE:  0.7674\n",
      "------------\n",
      "Fold 2\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.0019\n",
      "MAE:  0.7671\n",
      "------------\n",
      "Fold 3\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9812\n",
      "MAE:  0.7733\n",
      "------------\n",
      "------------\n",
      "Mean RMSE: 0.9904\n",
      "Mean MAE : 0.7693\n",
      "------------\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "# the accuracy of the tainset\n",
    "perf = evaluate(knn_default, dt, measures=['RMSE','MAE'], verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9995\n",
      "MAE:  0.7768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.77677919770828352"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy on test set\n",
    "rmse(defaultknn_prediction)\n",
    "mae(defaultknn_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9571428571428572"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coverage_ratio(defaultknn_prediction,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_knnprediction = mv_prediction(knn_default,missing_values)\n",
    "coverage_ratio(mv_knnprediction,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## changing hyper-paramater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "algo_name = KNNBasic\n",
    "#Define the different hyper-paramter range\n",
    "grid_param = {'k':[20,30,40,50,60],\n",
    "             'min_k':[1,5,10],\n",
    "             'sim_options':{'name':['msd','cosine','pearson'],\n",
    "                           'min_surpport':[0,3,5],\n",
    "                           'user_based':[False]}\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = surprise.GridSearch(algo_name, grid_param, measures=['RMSE', 'MAE'],verbose = 1)\n",
    "grid_search.evaluate(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params['RMSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score['RMSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params['MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score['MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### get the result of different hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "results_df = pd.DataFrame.from_dict(grid_search.cv_results)\n",
    "plotdata = results_df.loc[:,['MAE','RMSE','k','min_k']]\n",
    "name_list = []\n",
    "min_surppot_list = []\n",
    "for param in results_df.params:\n",
    "#     print(param)\n",
    "    name_list.append(param['sim_options']['name'])\n",
    "    min_surppot_list.append(param['sim_options']['min_surpport'])\n",
    "    \n",
    "name_list = pd.Series(name_list)\n",
    "min_surppot_list = pd.Series(min_surppot_list)\n",
    "\n",
    "plotdata['sim_matric'] = name_list\n",
    "plotdata['min_support'] = min_surppot_list\n",
    "plotdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdata_max_k = plotdata[plotdata.min_k ==1]\n",
    "plotdata_max_k = plotdata_max_k[plotdata_max_k.min_support == 3]\n",
    "plotdata_max_k = plotdata_max_k[plotdata_max_k.sim_matric == 'cosine']\n",
    "plotdata_max_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1) \n",
    "plt.plot(plotdata_max_k.k, plotdata_max_k.MAE, 'r')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(plotdata_max_k.k, plotdata_max_k.RMSE, 'g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdata_mink = plotdata[plotdata.sim_matric =='cosine']\n",
    "plotdata_mink = plotdata_mink[plotdata_mink.min_support == 3]\n",
    "plotdata_mink = plotdata_mink[plotdata_mink.k == 40]\n",
    "plotdata_mink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 2) \n",
    "plt.plot(plotdata_mink.min_k, plotdata_mink.MAE, 'r')\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(plotdata_mink.min_k, plotdata_mink.RMSE, 'g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdata_sim = plotdata[plotdata.min_k ==1]\n",
    "plotdata_sim = plotdata_sim[plotdata_sim.min_support == 0]\n",
    "plotdata_sim = plotdata_sim[plotdata_sim.k == 40]\n",
    "plotdata_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
